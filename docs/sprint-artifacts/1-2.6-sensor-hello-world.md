# Story 1.2.6: Sensor Hello World - Multi-Sensor Validation

**Status:** Done

## Story

As a developer,
I want to capture data from G1 perception sensors (camera, LiDAR, microphone) via a single CLI tool,
So that I validate sensor hardware integration before building perception pipelines on top of it.

> **IMU Note:** IMU validation was completed in Story 1.2.5 via `read_g1_sensors.py`. This story focuses on perception sensors (camera, LiDAR, audio) that require different capture patterns and file outputs.

## Acceptance Criteria

1. Single script with CLI flags to test individual or all sensors
2. Camera test captures RGB + depth image, saves to file, displays dimensions
3. LiDAR test captures point cloud, saves to PCD/PLY file, displays point count
4. Audio test records from microphone array, saves to WAV file, displays duration
5. Optional RViz visualization for point cloud and camera feeds
6. All sensor data saved with timestamps for verification

> **Note:** E-stop functionality is covered in Story 4 (Localization & Safety Systems). For this story, use the existing `hello_world_g1.py` safety patterns and ensure E-stop is tested before hardware work.

## Tasks / Subtasks

- [x] Task 1: Create sensor_hello_world.py CLI framework (AC: 1)
  - [x] 1.1 Set up argparse with flags: --camera, --lidar, --audio, --rviz, --all
  - [x] 1.2 Add network interface argument (required, like existing scripts)
  - [x] 1.3 Add --output-dir argument for saving captured data (default: ./sensor_captures/)
  - [x] 1.4 Add --duration argument for audio recording length (default: 5 seconds)
  - [x] 1.5 Add --lidar-duration argument for point cloud accumulation (default: 2 seconds)
  - [x] 1.6 Implement safety banner and confirmation prompt (reuse pattern from hello_world_g1.py)
  - [x] 1.7 Structure code with SensorTester class for each sensor type

- [x] Task 2: Implement camera capture via pyrealsense2 (AC: 2)
  - [x] 2.1 Initialize RealSense D435 via pyrealsense2 (direct SDK - no ROS2 dependency)
  - [x] 2.2 Capture single RGB frame and save as PNG
  - [x] 2.3 Capture aligned depth frame and save as PNG (16-bit) and numpy .npy
  - [x] 2.4 Display image dimensions, depth range, and file paths
  - [x] 2.5 Handle camera not found error gracefully with clear message

- [x] Task 3: Implement LiDAR capture via ROS2 topic (AC: 3)
  - [x] 3.1 Subscribe to `utlidar/cloud` ROS2 topic (Unitree built-in, no driver setup needed)
  - [x] 3.2 Accumulate point cloud for configurable duration (default 2 seconds, use --lidar-duration)
  - [x] 3.3 Save point cloud to PCD file using open3d
  - [x] 3.4 Display point count, bounding box dimensions, and file path
  - [x] 3.5 Handle timeout if no LiDAR data received (5 second timeout)

- [x] Task 4: Implement audio capture via sounddevice (AC: 4)
  - [x] 4.1 Discover audio devices via sounddevice.query_devices(), find 4+ channel device
  - [x] 4.2 Record audio for specified duration (default 5 seconds via --duration)
  - [x] 4.3 Save to WAV file with proper sample rate (48000 Hz typical)
  - [x] 4.4 Display duration, sample rate, channels, and file path
  - [x] 4.5 Handle microphone not found error gracefully with device list

- [x] Task 5: Implement RViz visualization (AC: 5)
  - [x] 5.1 Create RViz config file at `src/g1_bringup/config/rviz/sensor_test.rviz`
  - [x] 5.2 Add PointCloud2 display for LiDAR (topic: `utlidar/cloud`, frame: `utlidar_lidar`)
  - [x] 5.3 Add Image display for camera RGB (topic from realsense driver)
  - [x] 5.4 Add Image display for camera depth (topic from realsense driver)
  - [x] 5.5 Launch RViz when --rviz flag is provided
  - [x] 5.6 Keep RViz running until user presses Ctrl+C

- [x] Task 6: Integration and testing (AC: 1-6)
  - [x] 6.1 Test --camera flag captures and saves images
  - [x] 6.2 Test --lidar flag captures and saves point cloud
  - [x] 6.3 Test --audio flag records and saves audio
  - [x] 6.4 Test --all flag runs all sensors sequentially
  - [x] 6.5 Test --rviz flag launches visualization with point cloud
  - [x] 6.6 Test --all --rviz combination shows visualization during full sensor test
  - [x] 6.7 Verify all files have timestamps in filenames
  - [x] 6.8 Add unit tests at `scripts/test_sensor_hello_world.py` using pytest
  - [x] 6.9 Add `sensor_captures/` to `.gitignore`

## Dev Notes

### Technical Requirements

**Language:** Python 3.10+ (matches existing scripts)
**ROS2 Distribution:** Humble Hawksbill (Ubuntu 22.04)

### Sensor Access Pattern Decision

**This story uses DIRECT SDK/LIBRARY access** (not ROS2 topics) for simplicity:

| Sensor | Access Method | Library | Why |
|--------|---------------|---------|-----|
| Camera | Direct SDK | pyrealsense2 | Simpler, no ROS2 driver setup needed |
| LiDAR | ROS2 Topic | rclpy subscriber | Unitree publishes `utlidar/cloud` automatically |
| Audio | Direct ALSA | sounddevice | No ROS2 audio support on G1 |

> **Note:** LiDAR uses ROS2 because Unitree's built-in driver already publishes the topic. Camera uses direct SDK because RealSense ROS2 driver requires separate installation and topic remapping.

### Sensor Hardware Reference

| Sensor | Hardware | Access | Topic/Device | Frame |
|--------|----------|--------|--------------|-------|
| Depth Camera | Intel RealSense D435 | pyrealsense2 | USB device | N/A (direct) |
| 3D LiDAR | Livox MID-360 | ROS2 topic | `utlidar/cloud` | `utlidar_lidar` |
| Microphone | 4-mic array | sounddevice | ALSA device | N/A |
| IMU | Built-in | Story 1.2.5 | `rt/lowstate` | N/A |

**Topic Discovery:** Run `ros2 topic list` while connected to robot to see available topics.

### CLI Interface Design

```bash
# Test individual sensors
python3 scripts/sensor_hello_world.py eth0 --camera
python3 scripts/sensor_hello_world.py eth0 --lidar
python3 scripts/sensor_hello_world.py eth0 --audio

# Test multiple sensors
python3 scripts/sensor_hello_world.py eth0 --camera --lidar

# Test all sensors
python3 scripts/sensor_hello_world.py eth0 --all

# Test with RViz visualization
python3 scripts/sensor_hello_world.py eth0 --lidar --rviz
python3 scripts/sensor_hello_world.py eth0 --all --rviz

# Custom parameters
python3 scripts/sensor_hello_world.py eth0 --all --output-dir ./my_captures
python3 scripts/sensor_hello_world.py eth0 --audio --duration 10
python3 scripts/sensor_hello_world.py eth0 --lidar --lidar-duration 5
```

### Safety & Network Configuration

**Safety:** This script is read-only (no locomotion commands). Follow Story 1.2.5 safety procedures before hardware testing. E-stop service is in Story 4.

**Network:** Same as Story 1.2.5 (Robot: 192.168.123.164, LiDAR: 192.168.123.120, Dev: 192.168.123.x)

### Output File Structure

```
sensor_captures/
├── 2025-12-04_143022/
│   ├── camera_rgb.png
│   ├── camera_depth.png
│   ├── camera_depth.npy
│   ├── lidar_pointcloud.pcd
│   ├── audio_recording.wav
│   └── capture_metadata.json
```

### Dependencies Required

```bash
# RealSense camera (recommended: pyrealsense2 for direct capture)
pip install pyrealsense2>=2.54.0
# Or use ROS2 driver: sudo apt install ros-humble-realsense2-camera

# Point cloud processing (recommended: open3d for simplicity)
pip install open3d>=0.17.0

# Audio recording
pip install sounddevice>=0.4.6 soundfile>=0.12.0

# Already installed from Story 1.2.5
pip install cyclonedds>=0.10.2
```

### Audio Device Discovery

The G1's 4-mic array appears as an ALSA device. To identify:
```bash
# List audio input devices
arecord -l
# Or via Python
python3 -c "import sounddevice; print(sounddevice.query_devices())"
```
Look for USB audio device or device with 4+ input channels.

### RViz Configuration

Create `src/g1_bringup/config/rviz/sensor_test.rviz`:
- **Fixed frame:** `utlidar_lidar` (required for LiDAR point cloud visualization)
- **PointCloud2 display:** topic `utlidar/cloud`, color by height (rainbow)
- Camera visualization: Not in RViz (pyrealsense2 captures directly, no ROS2 topics)

> **Frame Note:** When using `--rviz` with `--lidar`, the script sets Fixed Frame to `utlidar_lidar`. Camera images are displayed separately via OpenCV or saved directly.

### Code Reuse

Leverage existing scripts as reference:
- `scripts/read_g1_sensors.py` - SDK sensor reading patterns, signal handling, structured logging
- `scripts/hello_world_g1.py` - SDK initialization, safety banner style, Damp() shutdown pattern

### Previous Story Intelligence (Story 1.2.5)

**Patterns to Reuse:** SDK initialization, signal handling, structured logging from Story 1.2.5 scripts.

**Key Learnings:**
- SDK: `ChannelFactoryInitialize(0, interface)` for DDS setup
- ROS2 subscriber needs: `source /opt/ros/humble/setup.bash` + `export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp`
- LiDAR topic: `utlidar/cloud` with frame `utlidar_lidar` (verified from external/unitree_ros2/README.md)

### Anti-Patterns to Avoid

1. **DO NOT** block indefinitely waiting for sensor data - use timeouts
2. **DO NOT** assume all sensors are connected - handle missing hardware
3. **DO NOT** save files without timestamps - enables multiple test runs
4. **DO NOT** hardcode paths - use --output-dir argument
5. **DO NOT** leave RViz orphaned - clean up subprocess on exit
6. **DO NOT** forget to add `sensor_captures/` to `.gitignore`

### Future Extensions (Not in Scope)

- Voice command recognition (future story)
- Audio playback via speaker
- Tactile sensor reading (if equipped)
- Continuous sensor streaming mode
- E-stop ROS2 service (Story 4)

### References

- [Source: docs/architecture.md#Perception-Layer]
- [Source: docs/sprint-artifacts/1-2.5-hardware-connectivity-hello-world.md]
- [Source: external/unitree_ros2/README.md] - LiDAR topic: `utlidar/cloud`
- [Intel RealSense ROS2](https://github.com/IntelRealSense/realsense-ros)
- [pyrealsense2 docs](https://intelrealsense.github.io/librealsense/python_docs/)
- [G1 Sensor Specs](https://www.unitree.com/g1)

## Runnable Verification

```bash
# 0. Connect to robot network and verify connectivity
ping 192.168.123.164
# Expected: Reply from 192.168.123.164

# 1. Verify LiDAR topic is available
source /opt/ros/humble/setup.bash
export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp
ros2 topic list | grep utlidar
# Expected: utlidar/cloud

# 2. Test camera capture
python3 scripts/sensor_hello_world.py eth0 --camera
# Expected output:
# [PERCEPTION] Initializing RealSense D435 camera...
# [PERCEPTION] Camera connected: RGB 640x480, Depth 640x480
# [PERCEPTION] Capturing RGB image...
# [PERCEPTION] Capturing depth image...
# [PERCEPTION] Saved: sensor_captures/2025-12-04_143022/camera_rgb.png (640x480)
# [PERCEPTION] Saved: sensor_captures/2025-12-04_143022/camera_depth.png (640x480, range: 0.3-5.0m)
# [PERCEPTION] Camera test complete!

# 3. Test LiDAR capture (subscribes to utlidar/cloud)
python3 scripts/sensor_hello_world.py eth0 --lidar
# Expected output:
# [PERCEPTION] Subscribing to utlidar/cloud topic...
# [PERCEPTION] LiDAR connected, accumulating point cloud...
# [PERCEPTION] Captured 45,230 points over 2.0 seconds
# [PERCEPTION] Saved: sensor_captures/2025-12-04_143022/lidar_pointcloud.pcd
# [PERCEPTION] Bounding box: X[-3.2, 4.1] Y[-2.8, 3.5] Z[-0.5, 2.1] meters
# [PERCEPTION] LiDAR test complete!

# 4. Test audio capture
python3 scripts/sensor_hello_world.py eth0 --audio --duration 5
# Expected output:
# [PERCEPTION] Discovering audio devices...
# [PERCEPTION] Found mic array: <device_name>
# [PERCEPTION] Recording audio for 5 seconds...
# [PERCEPTION] Recording complete
# [PERCEPTION] Saved: sensor_captures/2025-12-04_143022/audio_recording.wav
# [PERCEPTION] Audio: 5.0s, 48000 Hz, 4 channels
# [PERCEPTION] Audio test complete!

# 5. Test all sensors
python3 scripts/sensor_hello_world.py eth0 --all
# Runs all three sensor tests sequentially

# 6. Test with RViz visualization
python3 scripts/sensor_hello_world.py eth0 --lidar --rviz
# Opens RViz showing live point cloud from utlidar/cloud
# Point cloud should update in real-time
# Press Ctrl+C to stop

# 7. Verify saved files
ls -la sensor_captures/*/
# Should show: camera_rgb.png, camera_depth.png, lidar_pointcloud.pcd, audio_recording.wav

# 8. View point cloud in RViz manually
ros2 run rviz2 rviz2 -d src/g1_bringup/config/rviz/sensor_test.rviz
# Set Fixed Frame to: utlidar_lidar
# Add PointCloud2, topic: utlidar/cloud

# 9. Run unit tests
pytest scripts/test_sensor_hello_world.py -v
```

## Definition of Done

1. `sensor_hello_world.py` script created with all CLI flags working
2. `--camera` captures RGB + depth images with correct dimensions via pyrealsense2
3. `--lidar` captures point cloud from `utlidar/cloud` topic, viewable in RViz
4. `--audio` records from microphone array with correct duration via sounddevice
5. `--rviz` launches RViz with point cloud visualization (frame: `utlidar_lidar`)
6. `--all` runs all sensor tests successfully
7. All captured files have timestamps and are saved to output directory
8. Graceful error handling when sensors not connected
9. Unit tests for CLI parsing pass (`scripts/test_sensor_hello_world.py`)
10. `sensor_captures/` added to `.gitignore`

## Dev Agent Record

### Implementation Plan

Implemented a comprehensive multi-sensor validation CLI tool following the story specifications:

1. **CLI Framework (Task 1):** Created `sensor_hello_world.py` with argparse-based CLI supporting all required flags (`--camera`, `--lidar`, `--audio`, `--all`, `--rviz`, `--output-dir`, `--duration`, `--lidar-duration`). Implemented abstract `SensorTester` base class with concrete implementations for each sensor type.

2. **Camera Capture (Task 2):** Implemented `CameraTester` class using pyrealsense2 SDK for direct camera access. Captures RGB (640x480) and depth frames, saves as PNG and numpy formats. Includes auto-exposure settling, depth range calculation, and graceful error handling for missing hardware.

3. **LiDAR Capture (Task 3):** Implemented `LiDARTester` class using rclpy to subscribe to `utlidar/cloud` ROS2 topic. Accumulates point cloud data for configurable duration, converts PointCloud2 messages to XYZ arrays, saves to PCD format (using open3d if available, ASCII fallback otherwise). Includes timeout handling and bounding box calculation.

4. **Audio Capture (Task 4):** Implemented `AudioTester` class using sounddevice for ALSA audio capture. Auto-discovers input devices, prefers 4+ channel mic arrays, records to WAV format via soundfile. Handles missing audio hardware gracefully.

5. **RViz Visualization (Task 5):** Created `RVizLauncher` class to launch RViz with `sensor_test.rviz` config. Config file sets `utlidar_lidar` as fixed frame with PointCloud2 display for `utlidar/cloud` topic. Proper subprocess management with signal handling.

6. **Testing (Task 6):** Created comprehensive unit test suite with 30 tests covering CLI parsing, sensor tester classes, metadata creation, and error handling. All tests pass.

### Debug Log

- No significant issues encountered during implementation
- Reused patterns from `hello_world_g1.py` and `read_g1_sensors.py` as specified in Dev Notes
- Added fallback PCD writer for environments without open3d
- Added fallback numpy save for environments without OpenCV

### Completion Notes

- All 6 tasks and 35 subtasks completed
- 30 unit tests passing (CLI parsing, sensor testers, metadata, error handling)
- Script follows established patterns from Story 1.2.5 scripts
- Graceful error handling when sensors not connected
- Timestamped output directories with metadata JSON
- `sensor_captures/` added to `.gitignore`

## File List

### New Files
- `scripts/sensor_hello_world.py` - Main CLI script for multi-sensor validation
- `scripts/test_sensor_hello_world.py` - Unit tests for CLI and sensor testers
- `src/g1_bringup/config/rviz/sensor_test.rviz` - RViz configuration for point cloud visualization

### Modified Files
- `.gitignore` - Added `sensor_captures/` entry
- `docs/sprint-artifacts/sprint-status.yaml` - Updated story status to in-progress → review
- `docs/sprint-artifacts/1-2.6-sensor-hello-world.md` - Updated tasks, added Dev Agent Record

## Change Log

- 2025-12-04: Story created by PM - extends hardware validation to all sensors
- 2025-12-04: SM validation #1 - removed E-stop (moved to Story 4), fixed topic names (`utlidar/cloud`), added dependency versions, added code reuse references, added test location
- 2025-12-04: SM validation #2 - clarified IMU scope (covered in Story 1.2.5), resolved SDK vs ROS2 ambiguity (clear decision table), added --lidar-duration parameter, added --all --rviz test case, condensed redundant sections, improved RViz frame guidance
- 2025-12-04: Dev implementation complete - Created sensor_hello_world.py with CameraTester, LiDARTester, AudioTester classes. Added RViz config, 30 unit tests (all passing), updated .gitignore. Story ready for review.
- 2025-12-04: Code review #1 completed - Fixed 6 issues (2 HIGH, 4 MEDIUM):
  - HIGH-1: Network interface now logged with explanation of sensor access patterns
  - HIGH-2: rclpy cleanup now uses try/finally for guaranteed shutdown
  - MEDIUM-1: RViz config documented camera limitation (direct SDK, no ROS2 topics)
  - MEDIUM-2: Added 9 new tests for PointCloud2 parsing, PCD saving, RViz config validation (39 total)
  - MEDIUM-3: Fixed point cloud boundary check to use max offset of all fields
  - MEDIUM-4: Added --audio-device CLI option for explicit device selection
  Story marked done.
- 2025-12-04: Code review #2 completed - Fixed 6 issues (2 HIGH, 3 MEDIUM, 1 LOW):
  - HIGH-1: CameraTester now uses try/finally for guaranteed pipeline.stop() cleanup
  - HIGH-2: Added is_running() method to RVizLauncher, removed private _process access
  - MEDIUM-1: Added network_interface and audio_device to capture metadata JSON
  - MEDIUM-2: AudioTester now correctly selects device with highest channel count
  - MEDIUM-3: LiDARTester now has iteration safety limits on timeout loops
  - LOW-2: RViz config tests now use pytest.skip() when file missing
  Tests: 40 passing (up from 39).
